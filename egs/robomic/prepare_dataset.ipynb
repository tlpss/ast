{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/icra2025-v0\" # root dir of your dataset \n",
    "\"\"\" / \n",
    "        ├── cat1\n",
    "        │   ├── <sensor>_<timestamp>.csv\n",
    "        │   ├── <sensor>_<timestamp>.pkl\n",
    "        │   ├── ...\n",
    "        ├── cat2\n",
    "        │   ├── <sensor>_<timestamp>.csv\n",
    "        │   ├── <sensor>_<timestamp>.pkl\n",
    "        │   ├── ...\n",
    "        ├── ... \n",
    "\"\"\"\n",
    "\n",
    "categories_to_drop = [\"empty_disturbed\"]\n",
    "\n",
    "sample_freq_to_encode= 16000 # 16kHz\n",
    "store_categories_csv = False # create a csv that maps each category to an index, only do this once and make sure it is consistent across all datasets!\n",
    "N_FOLDS = 5 # set to zero if you do not want to create train/val splits. \n",
    "val_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all \"wav\" files from the data directory to avoid inconsistencies\n",
    "import os\n",
    "import glob\n",
    "# glob cannot find files with a leading dot\n",
    "files = glob.glob(os.path.join(DATA_DIR, \"**\", \"*.wav\"), recursive=True)\n",
    "\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3_M6x14', 'empty', 'empty_disturbed']\n",
      "3_M6x14\n",
      "['laser_2025-02-18[15].csv', 'mic_2025-02-18[20].csv', 'mic_2025-02-18[11].csv', 'laser_2025-02-18[9].csv', 'laser_2025-02-18[17].csv', 'laser_2025-02-18[14].csv', 'mic_2025-02-18[17].csv', 'mic_2025-02-18[6].csv', 'laser_2025-02-18[16].csv', 'mic_2025-02-18[1].csv', 'laser_2025-02-18[18].csv', 'laser_2025-02-18[8].csv', 'mic_2025-02-18[2].csv', 'laser_2025-02-18[20].csv', 'mic_2025-02-18[5].csv', 'mic_2025-02-18[13].csv', 'laser_2025-02-18[12].csv', 'laser_2025-02-18[5].csv', 'laser_2025-02-18[19].csv', 'mic_2025-02-18[9].csv', 'laser_2025-02-18[3].csv', 'laser_2025-02-18[1].csv', 'mic_2025-02-18[7].csv', 'mic_2025-02-18[15].csv', 'mic_2025-02-18[10].csv', 'mic_2025-02-18[19].csv', 'laser_2025-02-18[6].csv', 'mic_2025-02-18[14].csv', 'mic_2025-02-18[16].csv', 'mic_2025-02-18[4].csv', 'laser_2025-02-18[4].csv', 'mic_2025-02-18[12].csv', 'laser_2025-02-18[10].csv', 'laser_2025-02-18[2].csv', 'laser_2025-02-18[13].csv', 'mic_2025-02-18[18].csv', 'laser_2025-02-18[11].csv', 'mic_2025-02-18[3].csv', 'mic_2025-02-18[8].csv', 'laser_2025-02-18[7].csv']\n",
      "empty\n",
      "['laser_2025-02-18[15].csv', 'mic_2025-02-18[20].csv', 'mic_2025-02-18[11].csv', 'laser_2025-02-18[9].csv', 'laser_2025-02-18[17].csv', 'laser_2025-02-18[14].csv', 'mic_2025-02-18[17].csv', 'mic_2025-02-18[6].csv', 'laser_2025-02-18[16].csv', 'mic_2025-02-18[1].csv', 'laser_2025-02-18[18].csv', 'laser_2025-02-18[8].csv', 'mic_2025-02-18[2].csv', 'laser_2025-02-18[20].csv', 'mic_2025-02-18[5].csv', 'mic_2025-02-18[13].csv', 'laser_2025-02-18[12].csv', 'laser_2025-02-18[5].csv', 'laser_2025-02-18[19].csv', 'mic_2025-02-18[9].csv', 'laser_2025-02-18[3].csv', 'laser_2025-02-18[1].csv', 'mic_2025-02-18[7].csv', 'mic_2025-02-18[15].csv', 'mic_2025-02-18[10].csv', 'mic_2025-02-18[19].csv', 'laser_2025-02-18[6].csv', 'mic_2025-02-18[14].csv', 'mic_2025-02-18[16].csv', 'mic_2025-02-18[4].csv', 'laser_2025-02-18[4].csv', 'mic_2025-02-18[12].csv', 'laser_2025-02-18[10].csv', 'laser_2025-02-18[2].csv', 'laser_2025-02-18[13].csv', 'mic_2025-02-18[18].csv', 'laser_2025-02-18[11].csv', 'mic_2025-02-18[3].csv', 'mic_2025-02-18[8].csv', 'laser_2025-02-18[7].csv']\n",
      "empty_disturbed\n",
      "['mic_2025-02-18[1].csv', 'mic_2025-02-18[2].csv', 'laser_2025-02-18[3].csv', 'laser_2025-02-18[1].csv', 'laser_2025-02-18[2].csv', 'mic_2025-02-18[3].csv']\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "# find all csv files in the data directory, create dataframe\n",
    "categories = os.listdir(DATA_DIR)\n",
    "categories = [c for c in categories if os.path.isdir(os.path.join(DATA_DIR, c))]\n",
    "print(categories)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create df with 3 columns: filepath, sensor_type, category\n",
    "entries = []\n",
    "\n",
    "# for each category, find all csv files\n",
    "for category in categories:\n",
    "    print(category)\n",
    "    csv_files = os.listdir(os.path.join(DATA_DIR, category))\n",
    "    csv_files = [f for f in csv_files if f.endswith(\".csv\")]\n",
    "    print(csv_files)\n",
    "\n",
    "    for file in csv_files:\n",
    "        sensor_type = file.split(\"_\")[0]\n",
    "        filepath = os.path.join(DATA_DIR, category, file)\n",
    "        # add to df\n",
    "        entries.append({\"filepath\": filepath, \"sensor_type\": sensor_type, \"category\": category})    \n",
    "\n",
    "df = pd.DataFrame(entries)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset contains 80 entries\n"
     ]
    }
   ],
   "source": [
    "# drop categories if needed\n",
    "\n",
    "for cat in categories_to_drop:\n",
    "    df = df[df[\"category\"] != cat]\n",
    "print(f\"dataset contains {len(df)} entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:40<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# for each entry, read the csv file save wav file\n",
    "from converter import SpectrogramCalculator\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    " \n",
    "    filepath = row[\"filepath\"]\n",
    "    par_dir = os.path.dirname(filepath)\n",
    "    filename = os.path.basename(filepath)\n",
    "    filename = filename.replace(\".csv\", \"\")\n",
    "\n",
    "\n",
    "    calc = SpectrogramCalculator(par_dir, filename)\n",
    "    calc.encode_as_wav(16000) # AST was pretrained on 16kHz audio\n",
    "\n",
    "    # plot waveform\n",
    "    # plot_waveform(torch.tensor(calc.data).unsqueeze(0), int(calc.fs))\n",
    "    # plt.show()\n",
    "\n",
    "    # save RAM\n",
    "    del calc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2 categories\n"
     ]
    }
   ],
   "source": [
    "if store_categories_csv:\n",
    "    # create csv with categories\n",
    "    input(\"are you sure you want to create a csv with categories? This should only be done once and should be consistent across all datasets. Press enter to continue\")\n",
    "\n",
    "    categories = df[\"category\"].unique()\n",
    "    categories = {cat: i for i, cat in enumerate(categories)}\n",
    "\n",
    "    # create csv with entries\n",
    "    # idx, midname, category_name\n",
    "    print(categories)\n",
    "\n",
    "    with open(\"data/robomic_categories.csv\",\"w\") as f:\n",
    "        f.write(\"index,mid,display_name\\n\")\n",
    "        for name, index in categories.items():\n",
    "            midname = f\"m/robomic{index:02}\"\n",
    "            f.write(f'{index},{midname},\"{name}\"\\n')\n",
    "\n",
    "\n",
    "\n",
    "categories = open(\"data/robomic_categories.csv\").readlines()\n",
    "categories = [c.strip().split(\",\") for c in categories]\n",
    "categories = categories[1:]\n",
    "cat_name_to_idx = {c[2].replace('\"',''): int(c[0]) for c in categories}\n",
    "cat_idx_to_midname = {int(c[0]): c[1] for c in categories}\n",
    "print(f\"found {len(cat_name_to_idx)} categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to create json file that defines dataset\n",
    "import json\n",
    "def create_json_from_df(df, filename):\n",
    "\n",
    "    data = []\n",
    "    for i, row in df.iterrows():\n",
    "        wav_path = row[\"filepath\"].replace(\".csv\", \".wav\")\n",
    "        # make relative to filename directory\n",
    "        par_dir = os.path.dirname(filename)\n",
    "        wav_path = os.path.relpath(wav_path, par_dir)\n",
    "        category = row[\"category\"]\n",
    "        category_id = cat_name_to_idx[category]\n",
    "        midname_label = cat_idx_to_midname[category_id]\n",
    "        data.append({\"wav\": wav_path, \"labels\": midname_label})\n",
    "    json_data = {\"data\": data}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(json_data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by sensor type\n",
    "laser_df = df[df[\"sensor_type\"] == \"laser\"]\n",
    "mic_df = df[df[\"sensor_type\"] == \"mic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store entire dataset \n",
    "create_json_from_df(laser_df, f\"{DATA_DIR}/robomic_all_laser.json\")\n",
    "create_json_from_df(mic_df, f\"{DATA_DIR}/robomic_all_mic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "if N_FOLDS > 0:\n",
    "    for k in range(N_FOLDS):\n",
    "        # create train and test splits \n",
    "        train_laser_indices = random.sample(range(len(laser_df)), int(len(laser_df) * (1 - val_ratio)))\n",
    "        val_laser_indices = [i for i in range(len(laser_df)) if i not in train_laser_indices]\n",
    "\n",
    "        train_mic_indices = random.sample(range(len(mic_df)), int(len(mic_df) * (1 - val_ratio)))\n",
    "        val_mic_indices = [i for i in range(len(mic_df)) if i not in train_mic_indices]\n",
    "\n",
    "        train_laser_df = laser_df.iloc[train_laser_indices]\n",
    "        val_laser_df = laser_df.iloc[val_laser_indices]\n",
    "\n",
    "        train_mic_df = mic_df.iloc[train_mic_indices]\n",
    "        val_mic_df = mic_df.iloc[val_mic_indices]\n",
    "\n",
    "        create_json_from_df(train_laser_df, f\"{DATA_DIR}/robomic_train_laser_fold_{k}.json\")\n",
    "        create_json_from_df(val_laser_df, f\"{DATA_DIR}/robomic_val_laser_fold_{k}.json\")\n",
    "        create_json_from_df(train_mic_df, f\"{DATA_DIR}/robomic_train_mic_fold_{k}.json\")\n",
    "        create_json_from_df(val_mic_df, f\"{DATA_DIR}/robomic_val_mic_fold_{k}.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
